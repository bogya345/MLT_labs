{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download Alice in Wonderland by Lewis Carrol from Project Gutenberg's website \n",
    "(http://www.gutenberg.org/files/11/11-0.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = './data/AliceInWonderland.txt'\n",
    "# filePath = './data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file = open(filePath, 'r', encoding='utf8')\n",
    "org_text = file.read()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Perform any neccessary prepocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162611"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "text = org_text.lower()\n",
    "text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "text = re.sub(\"\\r\\n\", \"\", text)\n",
    "text = re.sub(\"\\t\", \"\", text)\n",
    "text = re.sub(\"\\\\n\", \"\", text)\n",
    "text = re.sub(\"_\", \"\", text)\n",
    "text = re.sub(\"\\s+\", \" \", text)\n",
    "text\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149303"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = text.split(\"chapter\")\n",
    "# chapters = re.split(r\"chapter.*([ivx]+)\", text)\n",
    "del chapters[0]\n",
    "# chapters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26627"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13530"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [i for i in words if i not in stopwords.words()]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(i) for i in words]\n",
    "[stemmer.stem(i) for i in words]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 words in a chapter 1\n",
      "\tWord: pictur, TF-IDF: 0.32855\n",
      "\tWord: sister, TF-IDF: 0.30276\n",
      "\tWord: book, TF-IDF: 0.26455\n",
      "\tWord: get, TF-IDF: 0.21331\n",
      "\tWord: idown, TF-IDF: 0.2024\n",
      "\tWord: rabbitholealic, TF-IDF: 0.2024\n",
      "\tWord: thebank, TF-IDF: 0.2024\n",
      "\tWord: orconvers, TF-IDF: 0.2024\n",
      "\tWord: alicewithout, TF-IDF: 0.2024\n",
      "\tWord: conversationsso, TF-IDF: 0.2024\n",
      "==================================================\n",
      "Top-10 words in a chapter 2\n",
      "\tWord: far, TF-IDF: 0.26432\n",
      "\tWord: foot, TF-IDF: 0.2438\n",
      "\tWord: tearscurious, TF-IDF: 0.18652\n",
      "\tWord: thatfor, TF-IDF: 0.18652\n",
      "\tWord: imopen, TF-IDF: 0.18652\n",
      "\tWord: largest, TF-IDF: 0.18652\n",
      "\tWord: goodby, TF-IDF: 0.18652\n",
      "\tWord: feetfor, TF-IDF: 0.18652\n",
      "\tWord: ofsight, TF-IDF: 0.18652\n",
      "\tWord: stock, TF-IDF: 0.18652\n",
      "==================================================\n",
      "Top-10 words in a chapter 3\n",
      "\tWord: inde, TF-IDF: 0.26809\n",
      "\tWord: long, TF-IDF: 0.2286\n",
      "\tWord: iiia, TF-IDF: 0.19414\n",
      "\tWord: talethey, TF-IDF: 0.19414\n",
      "\tWord: queerlook, TF-IDF: 0.19414\n",
      "\tWord: bankthebird, TF-IDF: 0.19414\n",
      "\tWord: draggl, TF-IDF: 0.19414\n",
      "\tWord: cling, TF-IDF: 0.19414\n",
      "\tWord: closeto, TF-IDF: 0.19414\n",
      "\tWord: drip, TF-IDF: 0.19414\n",
      "==================================================\n",
      "Top-10 words in a chapter 4\n",
      "\tWord: white, TF-IDF: 0.24446\n",
      "\tWord: duchess, TF-IDF: 0.231\n",
      "\tWord: rabbit, TF-IDF: 0.22246\n",
      "\tWord: ivth, TF-IDF: 0.19025\n",
      "\tWord: billit, TF-IDF: 0.19025\n",
      "\tWord: heardit, TF-IDF: 0.19025\n",
      "\tWord: areferret, TF-IDF: 0.19025\n",
      "\tWord: kidglov, TF-IDF: 0.19025\n",
      "\tWord: butthey, TF-IDF: 0.19025\n",
      "\tWord: seeneveryth, TF-IDF: 0.19025\n",
      "==================================================\n",
      "Top-10 words in a chapter 5\n",
      "\tWord: caterpillar, TF-IDF: 0.35657\n",
      "\tWord: sir, TF-IDF: 0.29103\n",
      "\tWord: time, TF-IDF: 0.19638\n",
      "\tWord: said, TF-IDF: 0.19483\n",
      "\tWord: vadvic, TF-IDF: 0.18294\n",
      "\tWord: insil, TF-IDF: 0.18294\n",
      "\tWord: andaddress, TF-IDF: 0.18294\n",
      "\tWord: languid, TF-IDF: 0.18294\n",
      "\tWord: sleepi, TF-IDF: 0.18294\n",
      "==================================================\n",
      "Top-10 words in a chapter 6\n",
      "\tWord: footman, TF-IDF: 0.58499\n",
      "\tWord: vipig, TF-IDF: 0.2024\n",
      "\tWord: pepperfor, TF-IDF: 0.2024\n",
      "\tWord: whatto, TF-IDF: 0.2024\n",
      "\tWord: thewoodsh, TF-IDF: 0.2024\n",
      "\tWord: liveryotherwis, TF-IDF: 0.2024\n",
      "\tWord: afishand, TF-IDF: 0.2024\n",
      "\tWord: knuckl, TF-IDF: 0.2024\n",
      "\tWord: byanoth, TF-IDF: 0.2024\n",
      "\tWord: afrog, TF-IDF: 0.2024\n",
      "==================================================\n",
      "Top-10 words in a chapter 7\n",
      "\tWord: room, TF-IDF: 0.40135\n",
      "\tWord: tabl, TF-IDF: 0.25752\n",
      "\tWord: dormous, TF-IDF: 0.24266\n",
      "\tWord: viia, TF-IDF: 0.19818\n",
      "\tWord: teapartyther, TF-IDF: 0.19818\n",
      "\tWord: themarch, TF-IDF: 0.19818\n",
      "\tWord: sittingbetween, TF-IDF: 0.19818\n",
      "\tWord: acushion, TF-IDF: 0.19818\n",
      "\tWord: veryuncomfort, TF-IDF: 0.19818\n",
      "\tWord: asleepi, TF-IDF: 0.19818\n",
      "==================================================\n",
      "Top-10 words in a chapter 8\n",
      "\tWord: five, TF-IDF: 0.51264\n",
      "\tWord: seven, TF-IDF: 0.26092\n",
      "\tWord: garden, TF-IDF: 0.21843\n",
      "\tWord: said, TF-IDF: 0.17468\n",
      "\tWord: look, TF-IDF: 0.16721\n",
      "\tWord: viiith, TF-IDF: 0.16401\n",
      "\tWord: croquetgrounda, TF-IDF: 0.16401\n",
      "\tWord: entranc, TF-IDF: 0.16401\n",
      "\tWord: rosesgrow, TF-IDF: 0.16401\n",
      "\tWord: busilypaint, TF-IDF: 0.16401\n",
      "==================================================\n",
      "Top-10 words in a chapter 9\n",
      "\tWord: pepper, TF-IDF: 0.50681\n",
      "\tWord: glad, TF-IDF: 0.3234\n",
      "\tWord: duchess, TF-IDF: 0.2625\n",
      "\tWord: ixth, TF-IDF: 0.2162\n",
      "\tWord: storyy, TF-IDF: 0.2162\n",
      "\tWord: thingsaid, TF-IDF: 0.2162\n",
      "\tWord: affection, TF-IDF: 0.2162\n",
      "\tWord: alicesand, TF-IDF: 0.2162\n",
      "\tWord: thoughtto, TF-IDF: 0.2162\n",
      "\tWord: sosavag, TF-IDF: 0.2162\n",
      "==================================================\n",
      "Top-10 words in a chapter 10\n",
      "\tWord: back, TF-IDF: 0.22399\n",
      "\tWord: voic, TF-IDF: 0.21465\n",
      "\tWord: mock, TF-IDF: 0.21105\n",
      "\tWord: turtl, TF-IDF: 0.20776\n",
      "\tWord: quadrilleth, TF-IDF: 0.18294\n",
      "\tWord: acrosshi, TF-IDF: 0.18294\n",
      "\tWord: throatsaid, TF-IDF: 0.18294\n",
      "\tWord: punch, TF-IDF: 0.18294\n",
      "\tWord: tearsrun, TF-IDF: 0.18294\n",
      "\tWord: cheek, TF-IDF: 0.18294\n",
      "==================================================\n",
      "Top-10 words in a chapter 11\n",
      "\tWord: xiwho, TF-IDF: 0.19818\n",
      "\tWord: tartsth, TF-IDF: 0.19818\n",
      "\tWord: throne, TF-IDF: 0.19818\n",
      "\tWord: theyarriv, TF-IDF: 0.19818\n",
      "\tWord: littlebird, TF-IDF: 0.19818\n",
      "\tWord: wasstand, TF-IDF: 0.19818\n",
      "\tWord: guardhim, TF-IDF: 0.19818\n",
      "\tWord: onehand, TF-IDF: 0.19818\n",
      "\tWord: sogood, TF-IDF: 0.19818\n",
      "\tWord: seat, TF-IDF: 0.18374\n",
      "==================================================\n",
      "Top-10 words in a chapter 12\n",
      "\tWord: xiialic, TF-IDF: 0.2024\n",
      "\tWord: evidenceher, TF-IDF: 0.2024\n",
      "\tWord: howlarg, TF-IDF: 0.2024\n",
      "\tWord: sucha, TF-IDF: 0.2024\n",
      "\tWord: skirtupset, TF-IDF: 0.2024\n",
      "\tWord: therethey, TF-IDF: 0.2024\n",
      "\tWord: sprawl, TF-IDF: 0.2024\n",
      "\tWord: remind, TF-IDF: 0.2024\n",
      "\tWord: globe, TF-IDF: 0.2024\n",
      "\tWord: ofgoldfish, TF-IDF: 0.2024\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, chapter in enumerate(chapters):\n",
    "    # chapter = re.sub(r\"[^\\w\\s]\", \"\", chapter, re.UNICODE)\n",
    "    # chapter = chapter.lower()\n",
    "    # chapter = [lemmatizer.lemmatize(token) for token in TreebankWordTokenizer().tokenize(chapter)]\n",
    "    # chapter = [word for word in chapter if not word in stop_words]\n",
    "\n",
    "    # chapter = ' '.join([lemmatizer.lemmatize(i) for i in chapter.split(' ')])\n",
    "    # chapter = ' '.join([stemmer.stem(i) for i in chapter.split(' ')])\n",
    "\n",
    "    chapter = chapter.split(' ')[:100]\n",
    "    # chapter = chapter.split(' ')\n",
    "    chapter = [i for i in chapter if i not in stopwords.words()]\n",
    "    chapter = [lemmatizer.lemmatize(i) for i in chapter]\n",
    "    chapter = [stemmer.stem(i) for i in chapter]\n",
    "\n",
    "    analys = nltk.Text(chapter)\n",
    "    print(\"Top-10 words in a chapter\", idx+1)\n",
    "    scores = {word: nltk.TextCollection(words).tf_idf(word, chapter) for word in analys}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    counter = 0\n",
    "\n",
    "    for word, score in sorted_words[:10]:\n",
    "        # if word == \"wa\":\n",
    "        #     word = \"was\"\n",
    "        if word not in ('alice'):\n",
    "            print(f\"\\tWord: {word}, TF-IDF: {round(score, 5)}\")\n",
    "            counter += 1\n",
    "        if counter == 9:\n",
    "            continue\n",
    "        continue\n",
    "\n",
    "    print('='*50)\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find the top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = split_into_sentences(org_text)\n",
    "print(len(sentences))\n",
    "sentence_with_alice = []\n",
    "for sentence in sentences:\n",
    "    if re.search(r'\\bAlice\\b', sentence):\n",
    "        sentence_with_alice.append(sentence)\n",
    "# sentence_with_alice\n",
    "len(sentence_with_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "a = lambda x: lemmatizer.lemmatize(x[0], 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbdict = []\n",
    "for sentence in sentence_with_alice:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    verbs = filter(lambda x:is_verb(x[1]), pos_tagged)\n",
    "    verbdict.append((list(map(a, list(verbs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'sit',\n",
       " 'have',\n",
       " 'do',\n",
       " 'twice',\n",
       " 'have',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'read',\n",
       " 'have',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'do',\n",
       " 'think',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'think',\n",
       " 'occur',\n",
       " 'have',\n",
       " 'wonder',\n",
       " 'seem',\n",
       " '_took',\n",
       " 'look',\n",
       " 'hurry',\n",
       " 'start',\n",
       " 'flash',\n",
       " 'have',\n",
       " 'see',\n",
       " 'take',\n",
       " 'burn',\n",
       " 'run',\n",
       " 'be',\n",
       " 'see',\n",
       " 'pop',\n",
       " 'go',\n",
       " 'consider',\n",
       " 'be',\n",
       " 'get',\n",
       " 'go',\n",
       " 'dip',\n",
       " 'have',\n",
       " 'think',\n",
       " 'stop',\n",
       " 'find',\n",
       " 'fall',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'think',\n",
       " 'tumble',\n",
       " 'Let',\n",
       " 'see',\n",
       " 'be',\n",
       " 'think',\n",
       " 'see',\n",
       " 'have',\n",
       " 'learn',\n",
       " 'be',\n",
       " 'show',\n",
       " 'be',\n",
       " 'listen',\n",
       " 'be',\n",
       " 'say',\n",
       " 'wonder',\n",
       " 'get',\n",
       " 'have',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'say',\n",
       " 'be',\n",
       " 'do',\n",
       " 'begin',\n",
       " 'talk',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'go',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'eat',\n",
       " 'be',\n",
       " 'jump',\n",
       " 'look',\n",
       " 'be',\n",
       " 'be',\n",
       " 'be',\n",
       " 'hurry',\n",
       " 'be',\n",
       " 'be',\n",
       " 'lose',\n",
       " 'go',\n",
       " 'be',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'turn',\n",
       " 'get',\n",
       " 'be',\n",
       " 'round',\n",
       " 'be',\n",
       " 'lock',\n",
       " 'have',\n",
       " 'be',\n",
       " 'try',\n",
       " 'walk',\n",
       " 'wonder',\n",
       " 'be',\n",
       " 'get',\n",
       " 'come',\n",
       " 'make',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'belong',\n",
       " 'open',\n",
       " 'find',\n",
       " 'lead',\n",
       " 'kneel',\n",
       " 'look',\n",
       " 'saw',\n",
       " 'long',\n",
       " 'get',\n",
       " 'wander',\n",
       " 'get',\n",
       " 'go',\n",
       " 'think',\n",
       " 'be',\n",
       " 'see',\n",
       " 'have',\n",
       " 'happen',\n",
       " 'have',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'wait',\n",
       " 'go',\n",
       " 'hop',\n",
       " 'find',\n",
       " 'shut',\n",
       " 'find',\n",
       " 'be',\n",
       " 'say',\n",
       " 'round',\n",
       " 'be',\n",
       " 'print',\n",
       " 'be',\n",
       " 'say',\n",
       " 'be',\n",
       " 'go',\n",
       " 'do',\n",
       " '_that_',\n",
       " 'be',\n",
       " 'venture',\n",
       " 'taste',\n",
       " 'find',\n",
       " 'have',\n",
       " 'butter',\n",
       " 'finish',\n",
       " 'say',\n",
       " 'be',\n",
       " 'shut',\n",
       " 'wait',\n",
       " 'see',\n",
       " 'be',\n",
       " 'go',\n",
       " 'shrink',\n",
       " 'felt',\n",
       " 'end',\n",
       " 'know',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'go',\n",
       " 'find',\n",
       " 'happen',\n",
       " 'decide',\n",
       " 'go',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'advise',\n",
       " 'leave',\n",
       " 'think',\n",
       " 'pretend',\n",
       " 'be',\n",
       " 'say',\n",
       " 'make',\n",
       " 'reach',\n",
       " 'make',\n",
       " 'grow',\n",
       " 'creep',\n",
       " 'don',\n",
       " 'happen',\n",
       " 'hold',\n",
       " 'feel',\n",
       " 'be',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'find',\n",
       " 'remain',\n",
       " 'be',\n",
       " 'happen',\n",
       " 'cake',\n",
       " 'have',\n",
       " 'get',\n",
       " 'expect',\n",
       " 'happen',\n",
       " 'seem',\n",
       " 'go',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'surprise',\n",
       " 'forget',\n",
       " 'speak',\n",
       " 'open',\n",
       " 'be',\n",
       " 'be',\n",
       " 'manage',\n",
       " 'but',\n",
       " 'be',\n",
       " 'win',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'go',\n",
       " 'be',\n",
       " 'ashamed',\n",
       " 'say',\n",
       " 'say',\n",
       " 'go',\n",
       " 'cry',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'ask',\n",
       " 'come',\n",
       " 'begin',\n",
       " 'please',\n",
       " 'start',\n",
       " 'drop',\n",
       " 'skurried',\n",
       " 'go',\n",
       " 'take',\n",
       " 'be',\n",
       " 'keep',\n",
       " 'fan',\n",
       " 'go',\n",
       " 'talk',\n",
       " 'be',\n",
       " 'say',\n",
       " 'fill',\n",
       " 'go',\n",
       " 'be',\n",
       " 'have',\n",
       " 'go',\n",
       " 'live',\n",
       " 'have',\n",
       " 'next',\n",
       " 'play',\n",
       " 'cry',\n",
       " 'do',\n",
       " '_would_',\n",
       " 'put',\n",
       " 'say',\n",
       " 'frighten',\n",
       " 'find',\n",
       " 'have',\n",
       " 'be',\n",
       " 'have',\n",
       " 'come',\n",
       " 'go',\n",
       " 'find',\n",
       " 'bath',\n",
       " 'dig',\n",
       " 'lodge',\n",
       " 'say',\n",
       " 'swim',\n",
       " 'try',\n",
       " 'find',\n",
       " 'be',\n",
       " 'think',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'have',\n",
       " 'do',\n",
       " 'remember',\n",
       " 'have',\n",
       " 'see',\n",
       " 'doesn',\n",
       " 'think',\n",
       " 'daresay',\n",
       " 'come',\n",
       " 'have',\n",
       " 'have',\n",
       " 'happen',\n",
       " 'cry',\n",
       " 'afraid',\n",
       " 'have',\n",
       " 'hurt',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'go',\n",
       " 'herself',\n",
       " 'swim',\n",
       " 'sit',\n",
       " 'purr',\n",
       " 'lick',\n",
       " 'wash',\n",
       " 'be',\n",
       " 'nurseand',\n",
       " 'catch',\n",
       " 'beg',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'bristle',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'offend',\n",
       " 'say',\n",
       " 'change',\n",
       " 'do',\n",
       " 'answer',\n",
       " 'go',\n",
       " 'be',\n",
       " 'like',\n",
       " 'show',\n",
       " 'cry',\n",
       " 'offend',\n",
       " 'turn',\n",
       " 'be',\n",
       " 'think',\n",
       " 'say',\n",
       " 'get',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'hate',\n",
       " 'lead',\n",
       " 'be',\n",
       " 'get',\n",
       " 'have',\n",
       " 'seem',\n",
       " 'find',\n",
       " 'talk',\n",
       " 'have',\n",
       " 'know',\n",
       " 'have',\n",
       " 'say',\n",
       " 'be',\n",
       " 'know',\n",
       " 'allow',\n",
       " 'know',\n",
       " 'be',\n",
       " 'refuse',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'be',\n",
       " 'say',\n",
       " 'keep',\n",
       " 'fix',\n",
       " 'felt',\n",
       " 'catch',\n",
       " 'do',\n",
       " 'get',\n",
       " 'continue',\n",
       " 'turn',\n",
       " 'speak',\n",
       " 'say',\n",
       " 'doesn',\n",
       " 'seem',\n",
       " 'dry',\n",
       " 'say',\n",
       " 'want',\n",
       " 'know',\n",
       " 'have',\n",
       " 'pause',\n",
       " 'think',\n",
       " 'speak',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'say',\n",
       " 'point',\n",
       " 'crowd',\n",
       " 'call',\n",
       " 'have',\n",
       " 'do',\n",
       " 'put',\n",
       " 'pull',\n",
       " 'have',\n",
       " 'get',\n",
       " 'hand',\n",
       " 'go',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'think',\n",
       " 'look',\n",
       " 'do',\n",
       " 'dare',\n",
       " 'think',\n",
       " 'say',\n",
       " 'bow',\n",
       " 'take',\n",
       " 'look',\n",
       " 'promise',\n",
       " 'tell',\n",
       " 'know',\n",
       " 'say',\n",
       " 'be',\n",
       " 'add',\n",
       " 'afraid',\n",
       " 'be',\n",
       " 'offend',\n",
       " 'say',\n",
       " 'turn',\n",
       " '_is_',\n",
       " 'say',\n",
       " 'look',\n",
       " 'do',\n",
       " 'call',\n",
       " 'say',\n",
       " 'beg',\n",
       " 'say',\n",
       " 'have',\n",
       " 'get',\n",
       " 'think',\n",
       " 'say',\n",
       " 'make',\n",
       " 'look',\n",
       " 'plead',\n",
       " 'call',\n",
       " 'join',\n",
       " 'please',\n",
       " 'do',\n",
       " 'say',\n",
       " 'address',\n",
       " 'reply',\n",
       " 'be',\n",
       " 'talk',\n",
       " 'move',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'cry',\n",
       " 'felt',\n",
       " 'guess',\n",
       " 'be',\n",
       " 'look',\n",
       " 'begin',\n",
       " 'hunt',\n",
       " 'be',\n",
       " 'be',\n",
       " 'seeneverything',\n",
       " 'seem',\n",
       " 'have',\n",
       " 'change',\n",
       " 'have',\n",
       " 'vanish',\n",
       " 'go',\n",
       " 'hunt',\n",
       " 'call',\n",
       " 'do',\n",
       " 'be',\n",
       " 'frighten',\n",
       " 'run',\n",
       " 'point',\n",
       " 'try',\n",
       " 'explain',\n",
       " 'have',\n",
       " 'make',\n",
       " 'queer',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'be',\n",
       " 'go',\n",
       " 'begin',\n",
       " 'fancy',\n",
       " 'happen',\n",
       " 'don',\n",
       " 'go',\n",
       " 'stop',\n",
       " 'begin',\n",
       " 'order',\n",
       " 'have',\n",
       " 'have',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'get',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'think',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'order',\n",
       " 'think',\n",
       " '_never_',\n",
       " 'get',\n",
       " 'be',\n",
       " 'know',\n",
       " 'be',\n",
       " 'come',\n",
       " 'look',\n",
       " 'tremble',\n",
       " 'shake',\n",
       " 'forget',\n",
       " 'be',\n",
       " 'have',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'come',\n",
       " 'try',\n",
       " 'open',\n",
       " 'open',\n",
       " 'be',\n",
       " 'press',\n",
       " 'prove',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'go',\n",
       " 'get',\n",
       " 'think',\n",
       " 'wait',\n",
       " 'fancy',\n",
       " 'hear',\n",
       " 'spread',\n",
       " 'make',\n",
       " 'be',\n",
       " 'hear',\n",
       " 'don',\n",
       " 'think',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'come',\n",
       " 'think',\n",
       " 'knowNo',\n",
       " 'fluster',\n",
       " 'tell',\n",
       " 'know',\n",
       " 'be',\n",
       " 'come',\n",
       " 'go',\n",
       " 'say',\n",
       " 'call',\n",
       " 'do',\n",
       " 'set',\n",
       " 'be',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'wonder',\n",
       " '_will_',\n",
       " 'do',\n",
       " 'begin',\n",
       " 'move',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'do',\n",
       " 'begin',\n",
       " 'have',\n",
       " 'doubt',\n",
       " 'come',\n",
       " 'rattle',\n",
       " 'hit',\n",
       " 'notice',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'lay',\n",
       " 'come',\n",
       " 'make',\n",
       " 'appear',\n",
       " 'run',\n",
       " 'find',\n",
       " 'get',\n",
       " 'do',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'wander',\n",
       " 'be',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'find',\n",
       " 'say',\n",
       " 'coax',\n",
       " 'try',\n",
       " 'whistle',\n",
       " 'be',\n",
       " 'frighten',\n",
       " 'be',\n",
       " 'be',\n",
       " 'eat',\n",
       " 'know',\n",
       " 'do',\n",
       " 'pick',\n",
       " 'hold',\n",
       " 'jump',\n",
       " 'rush',\n",
       " 'make',\n",
       " 'worry',\n",
       " 'dodge',\n",
       " 'keep',\n",
       " 'be',\n",
       " 'run',\n",
       " 'appear',\n",
       " 'make',\n",
       " 'tumble',\n",
       " 'get',\n",
       " 'think',\n",
       " 'be',\n",
       " 'have',\n",
       " 'expect',\n",
       " 'be',\n",
       " 'trample',\n",
       " 'run',\n",
       " 'begin',\n",
       " 'run',\n",
       " 'bark',\n",
       " 'sit',\n",
       " 'pant',\n",
       " 'hang',\n",
       " 'seem',\n",
       " 'Alice',\n",
       " 'make',\n",
       " 'set',\n",
       " 'run',\n",
       " 'be',\n",
       " 'till',\n",
       " 'sound',\n",
       " 'say',\n",
       " 'lean',\n",
       " 'rest',\n",
       " 'fan',\n",
       " 'have',\n",
       " 'like',\n",
       " 'teach',\n",
       " 'trick',\n",
       " 'd',\n",
       " 'be',\n",
       " 'do',\n",
       " 'look',\n",
       " 'do',\n",
       " 'see',\n",
       " 'look',\n",
       " 'eat',\n",
       " 'drink',\n",
       " 'look',\n",
       " 'take',\n",
       " 'address',\n",
       " 'reply',\n",
       " 'know',\n",
       " 'know',\n",
       " '_was_',\n",
       " 'get',\n",
       " 'think',\n",
       " 'have',\n",
       " 'be',\n",
       " 'change',\n",
       " 'say',\n",
       " 'see',\n",
       " 'put',\n",
       " 'reply',\n",
       " 'begin',\n",
       " 'be',\n",
       " 'be',\n",
       " 'haven',\n",
       " 'find',\n",
       " 'say',\n",
       " 'have',\n",
       " 'turn',\n",
       " 'knowand',\n",
       " 'think',\n",
       " 'win',\n",
       " 'be',\n",
       " 'say',\n",
       " 'know',\n",
       " 'be',\n",
       " 'feel',\n",
       " '_me_',\n",
       " 'felt',\n",
       " 'make',\n",
       " 'draw',\n",
       " 'say',\n",
       " 'think',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'be',\n",
       " 'think',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'sound',\n",
       " 'turn',\n",
       " 'come',\n",
       " 'say',\n",
       " 'swallow',\n",
       " 'think',\n",
       " 'have',\n",
       " 'do',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'say',\n",
       " 'remember',\n",
       " 'usedand',\n",
       " 'don',\n",
       " 'keep',\n",
       " 'reply',\n",
       " 'fold',\n",
       " 'begin',\n",
       " 'be',\n",
       " 'say',\n",
       " 'have',\n",
       " 'become',\n",
       " 'stand',\n",
       " 'Do',\n",
       " 'think',\n",
       " 'be',\n",
       " '_quite_',\n",
       " 'say',\n",
       " 'have',\n",
       " 'get',\n",
       " 'alter',\n",
       " 'particular',\n",
       " 'reply',\n",
       " 'change',\n",
       " 'know',\n",
       " 'say',\n",
       " 'have',\n",
       " 'be',\n",
       " 'contradict',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'lose',\n",
       " 'like',\n",
       " 'be',\n",
       " 'wouldn',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'plead',\n",
       " 'wait',\n",
       " 'choose',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'remain',\n",
       " 'look',\n",
       " 'try',\n",
       " 'make',\n",
       " 'be',\n",
       " 'be',\n",
       " 'round',\n",
       " 'find',\n",
       " 'say',\n",
       " 'change',\n",
       " 'find',\n",
       " 'be',\n",
       " 'be',\n",
       " 'find',\n",
       " 'see',\n",
       " 'look',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'rise',\n",
       " 'lay',\n",
       " 'say',\n",
       " 'say',\n",
       " 'haven',\n",
       " 're',\n",
       " 'talk',\n",
       " 'say',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'say',\n",
       " 'have',\n",
       " 'finish',\n",
       " 'be',\n",
       " 'annoy',\n",
       " 'say',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'see',\n",
       " 'say',\n",
       " 'm',\n",
       " 'say',\n",
       " 'remember',\n",
       " 'have',\n",
       " 'go',\n",
       " '_have_',\n",
       " 'taste',\n",
       " 'say',\n",
       " 'be',\n",
       " 'do',\n",
       " 'know',\n",
       " 'be',\n",
       " 'be',\n",
       " 'give',\n",
       " 'add',\n",
       " 'look',\n",
       " 'know',\n",
       " 'do',\n",
       " 'matter',\n",
       " 're',\n",
       " 'matter',\n",
       " '_me_',\n",
       " 'say',\n",
       " 'look',\n",
       " 'happen',\n",
       " 'be',\n",
       " 'shouldn',\n",
       " 'want',\n",
       " 'don',\n",
       " 'raw',\n",
       " 'crouch',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'entangle',\n",
       " 'have',\n",
       " 'stop',\n",
       " 'untwist',\n",
       " 'live',\n",
       " 'think',\n",
       " 'll',\n",
       " 'do',\n",
       " 'come',\n",
       " '_this_',\n",
       " 'frighten',\n",
       " 'be',\n",
       " 'open',\n",
       " 'notice',\n",
       " 'have',\n",
       " 'powder',\n",
       " 'curl',\n",
       " 'laugh',\n",
       " 'have',\n",
       " 'run',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'go',\n",
       " 'be',\n",
       " 'sit',\n",
       " 'star',\n",
       " 'go',\n",
       " 'knock',\n",
       " 'say',\n",
       " 'be',\n",
       " 'get',\n",
       " 'be',\n",
       " 'look',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'ask',\n",
       " 'be',\n",
       " 'do',\n",
       " 'be',\n",
       " 'tell',\n",
       " 'say',\n",
       " 'talk',\n",
       " 'say',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'sneeze',\n",
       " 'tell',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'say',\n",
       " 'jump',\n",
       " 'saw',\n",
       " 'be',\n",
       " 'address',\n",
       " 'take',\n",
       " 'go',\n",
       " 'didn',\n",
       " 'know',\n",
       " 'grin',\n",
       " 'didn',\n",
       " 'know',\n",
       " '_could_',\n",
       " 'don',\n",
       " 'do',\n",
       " 'say',\n",
       " 'feel',\n",
       " 'please',\n",
       " 'have',\n",
       " 'get',\n",
       " 'do',\n",
       " 'think',\n",
       " 'be',\n",
       " 'introduce',\n",
       " 'cry',\n",
       " 'jump',\n",
       " '_not_',\n",
       " 'be',\n",
       " 'say',\n",
       " 'felt',\n",
       " 'get',\n",
       " 'show',\n",
       " 'glance',\n",
       " 'see',\n",
       " 'mean',\n",
       " 'take',\n",
       " 'be',\n",
       " 'stir',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'go',\n",
       " '_think_',\n",
       " 'be',\n",
       " 'twelve',\n",
       " 'sing',\n",
       " 'keep',\n",
       " 'toss',\n",
       " 'howl',\n",
       " 'hear',\n",
       " 'speak',\n",
       " 'beat',\n",
       " 'sneeze',\n",
       " 'thoroughly',\n",
       " 'enjoy',\n",
       " 'please',\n",
       " 'say',\n",
       " 'fling',\n",
       " 'speak',\n",
       " 'catch',\n",
       " 'be',\n",
       " 'hold',\n",
       " 'don',\n",
       " 'take',\n",
       " 'kill',\n",
       " 't',\n",
       " 'be',\n",
       " 'murder',\n",
       " 'leave',\n",
       " 'say',\n",
       " 'express',\n",
       " 'grunt',\n",
       " 'look',\n",
       " 'see',\n",
       " 'be',\n",
       " 'be',\n",
       " 'have',\n",
       " 'be',\n",
       " 'get',\n",
       " 'do',\n",
       " 're',\n",
       " 'go',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'have',\n",
       " 'do',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'be',\n",
       " 'do',\n",
       " 'get',\n",
       " 'grin',\n",
       " 'saw',\n",
       " 'please',\n",
       " 'go',\n",
       " 'don',\n",
       " 'say',\n",
       " 'so',\n",
       " 'get',\n",
       " 'add',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'deny',\n",
       " 'try',\n",
       " 'don',\n",
       " 'want',\n",
       " 'go',\n",
       " 'remark',\n",
       " 'say',\n",
       " 'think',\n",
       " 'prove',\n",
       " 'go',\n",
       " 'do',\n",
       " 'know',\n",
       " 're',\n",
       " 'suppose',\n",
       " 'say',\n",
       " 'purr',\n",
       " 'growl',\n",
       " 'say',\n",
       " 'like',\n",
       " 'say',\n",
       " 'haven',\n",
       " 'be',\n",
       " 'invite',\n",
       " 'be',\n",
       " 'surprise',\n",
       " 'be',\n",
       " 'get',\n",
       " 'use',\n",
       " 'queer',\n",
       " 'happen',\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = []\n",
    "for word_arr in verbdict:\n",
    "    for word in word_arr:\n",
    "        word = re.sub(r\"[^\\w\\s]\", \"\", word, re.UNICODE)\n",
    "        if word and word != \"s\":\n",
    "            verbs.append(word)\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 268),\n",
       " ('say', 198),\n",
       " ('have', 103),\n",
       " ('think', 74),\n",
       " ('go', 65),\n",
       " ('do', 48),\n",
       " ('get', 46),\n",
       " ('look', 44),\n",
       " ('begin', 32),\n",
       " ('know', 31)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actions she most often did\n",
    "fdist = FreqDist(nltk.Text(verbs))\n",
    "fdist.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1991ae5a1b0be0d62723ce64b41daebbd092507e11a2eda874c09458c9f791ee"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
