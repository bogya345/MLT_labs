{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download Alice in Wonderland by Lewis Carrol from Project Gutenberg's website \n",
    "(http://www.gutenberg.org/files/11/11-0.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = './data/AliceInWonderland.txt'\n",
    "# filePath = './data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file = open(filePath, 'r', encoding='utf8')\n",
    "org_text = file.read()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Perform any neccessary prepocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162611"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "text = org_text.lower()\n",
    "text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "text = re.sub(\"\\r\\n\", \"\", text)\n",
    "text = re.sub(\"\\t\", \"\", text)\n",
    "text = re.sub(\"\\\\n\", \"\", text)\n",
    "text = re.sub(\"_\", \"\", text)\n",
    "text = re.sub(\"\\s+\", \" \", text)\n",
    "text\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149303"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = text.split(\"chapter\")\n",
    "# chapters = re.split(r\"chapter.*([ivx]+)\", text)\n",
    "del chapters[0]\n",
    "# chapters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26627"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13530"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [i for i in words if i not in stopwords.words()]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(i) for i in words]\n",
    "[stemmer.stem(i) for i in words]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 words in Chapter 1\n",
      "\tWord: she, TF-IDF: 0.11302\n",
      "\tWord: to, TF-IDF: 0.10699\n",
      "\tWord: the, TF-IDF: 0.10292\n",
      "\tWord: and, TF-IDF: 0.08289\n",
      "\tWord: it, TF-IDF: 0.07998\n",
      "\tWord: wa, TF-IDF: 0.07994\n",
      "\tWord: of, TF-IDF: 0.07228\n",
      "\tWord: that, TF-IDF: 0.0544\n",
      "\tWord: veri, TF-IDF: 0.04788\n",
      "\tWord: for, TF-IDF: 0.04279\n",
      "==================================================\n",
      "Top-10 words in Chapter 2\n",
      "\tWord: the, TF-IDF: 0.11691\n",
      "\tWord: and, TF-IDF: 0.11107\n",
      "\tWord: she, TF-IDF: 0.10004\n",
      "\tWord: to, TF-IDF: 0.08504\n",
      "\tWord: wa, TF-IDF: 0.05491\n",
      "\tWord: of, TF-IDF: 0.05252\n",
      "\tWord: it, TF-IDF: 0.05144\n",
      "\tWord: in, TF-IDF: 0.04766\n",
      "\tWord: that, TF-IDF: 0.04681\n",
      "==================================================\n",
      "Top-10 words in Chapter 3\n",
      "\tWord: the, TF-IDF: 0.13585\n",
      "\tWord: and, TF-IDF: 0.11228\n",
      "\tWord: to, TF-IDF: 0.09513\n",
      "\tWord: said, TF-IDF: 0.07874\n",
      "\tWord: it, TF-IDF: 0.0679\n",
      "\tWord: of, TF-IDF: 0.05818\n",
      "\tWord: mous, TF-IDF: 0.05265\n",
      "\tWord: you, TF-IDF: 0.05219\n",
      "\tWord: she, TF-IDF: 0.05208\n",
      "==================================================\n",
      "Top-10 words in Chapter 4\n",
      "\tWord: the, TF-IDF: 0.11481\n",
      "\tWord: and, TF-IDF: 0.10982\n",
      "\tWord: she, TF-IDF: 0.09351\n",
      "\tWord: to, TF-IDF: 0.09331\n",
      "\tWord: of, TF-IDF: 0.07936\n",
      "\tWord: it, TF-IDF: 0.06323\n",
      "\tWord: wa, TF-IDF: 0.04405\n",
      "\tWord: that, TF-IDF: 0.03998\n",
      "==================================================\n",
      "Top-10 words in Chapter 5\n",
      "\tWord: the, TF-IDF: 0.11864\n",
      "\tWord: said, TF-IDF: 0.09583\n",
      "\tWord: to, TF-IDF: 0.09546\n",
      "\tWord: and, TF-IDF: 0.08222\n",
      "\tWord: it, TF-IDF: 0.07128\n",
      "\tWord: of, TF-IDF: 0.06787\n",
      "\tWord: she, TF-IDF: 0.06718\n",
      "\tWord: wa, TF-IDF: 0.05073\n",
      "\tWord: that, TF-IDF: 0.0465\n",
      "==================================================\n",
      "Top-10 words in Chapter 6\n",
      "\tWord: the, TF-IDF: 0.13495\n",
      "\tWord: and, TF-IDF: 0.10416\n",
      "\tWord: it, TF-IDF: 0.08776\n",
      "\tWord: to, TF-IDF: 0.08651\n",
      "\tWord: said, TF-IDF: 0.07463\n",
      "\tWord: she, TF-IDF: 0.07395\n",
      "\tWord: of, TF-IDF: 0.06301\n",
      "\tWord: wa, TF-IDF: 0.05324\n",
      "\tWord: that, TF-IDF: 0.04848\n",
      "\tWord: you, TF-IDF: 0.04835\n",
      "==================================================\n",
      "Top-10 words in Chapter 7\n",
      "\tWord: the, TF-IDF: 0.154\n",
      "\tWord: said, TF-IDF: 0.09499\n",
      "\tWord: it, TF-IDF: 0.08492\n",
      "\tWord: and, TF-IDF: 0.07493\n",
      "\tWord: you, TF-IDF: 0.06975\n",
      "\tWord: to, TF-IDF: 0.0612\n",
      "\tWord: hatter, TF-IDF: 0.05872\n",
      "\tWord: dormous, TF-IDF: 0.0569\n",
      "\tWord: of, TF-IDF: 0.05488\n",
      "==================================================\n",
      "Top-10 words in Chapter 8\n",
      "\tWord: the, TF-IDF: 0.13284\n",
      "\tWord: and, TF-IDF: 0.11048\n",
      "\tWord: to, TF-IDF: 0.0893\n",
      "\tWord: she, TF-IDF: 0.07598\n",
      "\tWord: said, TF-IDF: 0.06846\n",
      "\tWord: it, TF-IDF: 0.06829\n",
      "\tWord: of, TF-IDF: 0.06691\n",
      "\tWord: queen, TF-IDF: 0.05648\n",
      "\tWord: wa, TF-IDF: 0.05122\n",
      "==================================================\n",
      "Top-10 words in Chapter 9\n",
      "\tWord: the, TF-IDF: 0.13664\n",
      "\tWord: said, TF-IDF: 0.09444\n",
      "\tWord: to, TF-IDF: 0.0908\n",
      "\tWord: and, TF-IDF: 0.08592\n",
      "\tWord: of, TF-IDF: 0.06471\n",
      "\tWord: she, TF-IDF: 0.06047\n",
      "\tWord: that, TF-IDF: 0.05683\n",
      "\tWord: it, TF-IDF: 0.0564\n",
      "\tWord: you, TF-IDF: 0.0559\n",
      "==================================================\n",
      "Top-10 words in Chapter 10\n",
      "\tWord: the, TF-IDF: 0.16792\n",
      "\tWord: and, TF-IDF: 0.09735\n",
      "\tWord: you, TF-IDF: 0.09299\n",
      "\tWord: said, TF-IDF: 0.07906\n",
      "\tWord: to, TF-IDF: 0.06513\n",
      "\tWord: mock, TF-IDF: 0.05946\n",
      "\tWord: of, TF-IDF: 0.05648\n",
      "\tWord: turtl, TF-IDF: 0.05578\n",
      "\tWord: it, TF-IDF: 0.04775\n",
      "\tWord: gryphon, TF-IDF: 0.04686\n",
      "==================================================\n",
      "Top-10 words in Chapter 11\n",
      "\tWord: the, TF-IDF: 0.1898\n",
      "\tWord: of, TF-IDF: 0.09052\n",
      "\tWord: and, TF-IDF: 0.08524\n",
      "\tWord: said, TF-IDF: 0.08164\n",
      "\tWord: to, TF-IDF: 0.06793\n",
      "\tWord: she, TF-IDF: 0.05463\n",
      "\tWord: wa, TF-IDF: 0.05406\n",
      "\tWord: king, TF-IDF: 0.04833\n",
      "\tWord: that, TF-IDF: 0.04718\n",
      "\tWord: with, TF-IDF: 0.04119\n",
      "==================================================\n",
      "Top-10 words in Chapter 12\n",
      "\tWord: the, TF-IDF: 0.15416\n",
      "\tWord: of, TF-IDF: 0.10803\n",
      "\tWord: to, TF-IDF: 0.08817\n",
      "\tWord: and, TF-IDF: 0.08692\n",
      "\tWord: you, TF-IDF: 0.07039\n",
      "\tWord: project, TF-IDF: 0.05678\n",
      "\tWord: work, TF-IDF: 0.05498\n",
      "\tWord: in, TF-IDF: 0.05388\n",
      "\tWord: with, TF-IDF: 0.04913\n",
      "\tWord: said, TF-IDF: 0.04731\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, chapter in enumerate(chapters):\n",
    "    # chapter = re.sub(r\"[^\\w\\s]\", \"\", chapter, re.UNICODE)\n",
    "    # chapter = chapter.lower()\n",
    "    # chapter = [lemmatizer.lemmatize(token) for token in TreebankWordTokenizer().tokenize(chapter)]\n",
    "    # chapter = [word for word in chapter if not word in stop_words]\n",
    "\n",
    "    # chapter = ' '.join([lemmatizer.lemmatize(i) for i in chapter.split(' ')])\n",
    "    # chapter = ' '.join([stemmer.stem(i) for i in chapter.split(' ')])\n",
    "\n",
    "    # chapter = chapter.split(' ')[:100]\n",
    "    chapter = chapter.split(' ')\n",
    "    chapter = [lemmatizer.lemmatize(i) for i in chapter]\n",
    "    chapter = [stemmer.stem(i) for i in chapter]\n",
    "\n",
    "    analys = nltk.Text(chapter)\n",
    "    print(\"Top-10 words in Chapter\", idx+1)\n",
    "    scores = {word: nltk.TextCollection(chapter).tf_idf(word, chapter) for word in analys}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    counter = 0\n",
    "\n",
    "    for word, score in sorted_words[:10]:\n",
    "        # if word == \"wa\":\n",
    "        #     word = \"was\"\n",
    "        if word not in ('alice'):\n",
    "            print(f\"\\tWord: {word}, TF-IDF: {round(score, 5)}\")\n",
    "            counter += 1\n",
    "        if counter == 9:\n",
    "            continue\n",
    "        continue\n",
    "\n",
    "    print('='*50)\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find the top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = split_into_sentences(org_text)\n",
    "print(len(sentences))\n",
    "sentence_with_alice = []\n",
    "for sentence in sentences:\n",
    "    if re.search(r'\\bAlice\\b', sentence):\n",
    "        sentence_with_alice.append(sentence)\n",
    "# sentence_with_alice\n",
    "len(sentence_with_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "a = lambda x: lemmatizer.lemmatize(x[0], 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbdict = []\n",
    "for sentence in sentence_with_alice:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    verbs = filter(lambda x:is_verb(x[1]), pos_tagged)\n",
    "    verbdict.append((list(map(a, list(verbs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'sit',\n",
       " 'have',\n",
       " 'do',\n",
       " 'twice',\n",
       " 'have',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'read',\n",
       " 'have',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'do',\n",
       " 'think',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'think',\n",
       " 'occur',\n",
       " 'have',\n",
       " 'wonder',\n",
       " 'seem',\n",
       " '_took',\n",
       " 'look',\n",
       " 'hurry',\n",
       " 'start',\n",
       " 'flash',\n",
       " 'have',\n",
       " 'see',\n",
       " 'take',\n",
       " 'burn',\n",
       " 'run',\n",
       " 'be',\n",
       " 'see',\n",
       " 'pop',\n",
       " 'go',\n",
       " 'consider',\n",
       " 'be',\n",
       " 'get',\n",
       " 'go',\n",
       " 'dip',\n",
       " 'have',\n",
       " 'think',\n",
       " 'stop',\n",
       " 'find',\n",
       " 'fall',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'think',\n",
       " 'tumble',\n",
       " 'Let',\n",
       " 'see',\n",
       " 'be',\n",
       " 'think',\n",
       " 'see',\n",
       " 'have',\n",
       " 'learn',\n",
       " 'be',\n",
       " 'show',\n",
       " 'be',\n",
       " 'listen',\n",
       " 'be',\n",
       " 'say',\n",
       " 'wonder',\n",
       " 'get',\n",
       " 'have',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'say',\n",
       " 'be',\n",
       " 'do',\n",
       " 'begin',\n",
       " 'talk',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'go',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'eat',\n",
       " 'be',\n",
       " 'jump',\n",
       " 'look',\n",
       " 'be',\n",
       " 'be',\n",
       " 'be',\n",
       " 'hurry',\n",
       " 'be',\n",
       " 'be',\n",
       " 'lose',\n",
       " 'go',\n",
       " 'be',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'turn',\n",
       " 'get',\n",
       " 'be',\n",
       " 'round',\n",
       " 'be',\n",
       " 'lock',\n",
       " 'have',\n",
       " 'be',\n",
       " 'try',\n",
       " 'walk',\n",
       " 'wonder',\n",
       " 'be',\n",
       " 'get',\n",
       " 'come',\n",
       " 'make',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'belong',\n",
       " 'open',\n",
       " 'find',\n",
       " 'lead',\n",
       " 'kneel',\n",
       " 'look',\n",
       " 'saw',\n",
       " 'long',\n",
       " 'get',\n",
       " 'wander',\n",
       " 'get',\n",
       " 'go',\n",
       " 'think',\n",
       " 'be',\n",
       " 'see',\n",
       " 'have',\n",
       " 'happen',\n",
       " 'have',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'wait',\n",
       " 'go',\n",
       " 'hop',\n",
       " 'find',\n",
       " 'shut',\n",
       " 'find',\n",
       " 'be',\n",
       " 'say',\n",
       " 'round',\n",
       " 'be',\n",
       " 'print',\n",
       " 'be',\n",
       " 'say',\n",
       " 'be',\n",
       " 'go',\n",
       " 'do',\n",
       " '_that_',\n",
       " 'be',\n",
       " 'venture',\n",
       " 'taste',\n",
       " 'find',\n",
       " 'have',\n",
       " 'butter',\n",
       " 'finish',\n",
       " 'say',\n",
       " 'be',\n",
       " 'shut',\n",
       " 'wait',\n",
       " 'see',\n",
       " 'be',\n",
       " 'go',\n",
       " 'shrink',\n",
       " 'felt',\n",
       " 'end',\n",
       " 'know',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'go',\n",
       " 'find',\n",
       " 'happen',\n",
       " 'decide',\n",
       " 'go',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'advise',\n",
       " 'leave',\n",
       " 'think',\n",
       " 'pretend',\n",
       " 'be',\n",
       " 'say',\n",
       " 'make',\n",
       " 'reach',\n",
       " 'make',\n",
       " 'grow',\n",
       " 'creep',\n",
       " 'don',\n",
       " 'happen',\n",
       " 'hold',\n",
       " 'feel',\n",
       " 'be',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'find',\n",
       " 'remain',\n",
       " 'be',\n",
       " 'happen',\n",
       " 'cake',\n",
       " 'have',\n",
       " 'get',\n",
       " 'expect',\n",
       " 'happen',\n",
       " 'seem',\n",
       " 'go',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'surprise',\n",
       " 'forget',\n",
       " 'speak',\n",
       " 'open',\n",
       " 'be',\n",
       " 'be',\n",
       " 'manage',\n",
       " 'but',\n",
       " 'be',\n",
       " 'win',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'go',\n",
       " 'be',\n",
       " 'ashamed',\n",
       " 'say',\n",
       " 'say',\n",
       " 'go',\n",
       " 'cry',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'ask',\n",
       " 'come',\n",
       " 'begin',\n",
       " 'please',\n",
       " 'start',\n",
       " 'drop',\n",
       " 'skurried',\n",
       " 'go',\n",
       " 'take',\n",
       " 'be',\n",
       " 'keep',\n",
       " 'fan',\n",
       " 'go',\n",
       " 'talk',\n",
       " 'be',\n",
       " 'say',\n",
       " 'fill',\n",
       " 'go',\n",
       " 'be',\n",
       " 'have',\n",
       " 'go',\n",
       " 'live',\n",
       " 'have',\n",
       " 'next',\n",
       " 'play',\n",
       " 'cry',\n",
       " 'do',\n",
       " '_would_',\n",
       " 'put',\n",
       " 'say',\n",
       " 'frighten',\n",
       " 'find',\n",
       " 'have',\n",
       " 'be',\n",
       " 'have',\n",
       " 'come',\n",
       " 'go',\n",
       " 'find',\n",
       " 'bath',\n",
       " 'dig',\n",
       " 'lodge',\n",
       " 'say',\n",
       " 'swim',\n",
       " 'try',\n",
       " 'find',\n",
       " 'be',\n",
       " 'think',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'have',\n",
       " 'do',\n",
       " 'remember',\n",
       " 'have',\n",
       " 'see',\n",
       " 'doesn',\n",
       " 'think',\n",
       " 'daresay',\n",
       " 'come',\n",
       " 'have',\n",
       " 'have',\n",
       " 'happen',\n",
       " 'cry',\n",
       " 'afraid',\n",
       " 'have',\n",
       " 'hurt',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'go',\n",
       " 'herself',\n",
       " 'swim',\n",
       " 'sit',\n",
       " 'purr',\n",
       " 'lick',\n",
       " 'wash',\n",
       " 'be',\n",
       " 'nurseand',\n",
       " 'catch',\n",
       " 'beg',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'bristle',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'offend',\n",
       " 'say',\n",
       " 'change',\n",
       " 'do',\n",
       " 'answer',\n",
       " 'go',\n",
       " 'be',\n",
       " 'like',\n",
       " 'show',\n",
       " 'cry',\n",
       " 'offend',\n",
       " 'turn',\n",
       " 'be',\n",
       " 'think',\n",
       " 'say',\n",
       " 'get',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'hate',\n",
       " 'lead',\n",
       " 'be',\n",
       " 'get',\n",
       " 'have',\n",
       " 'seem',\n",
       " 'find',\n",
       " 'talk',\n",
       " 'have',\n",
       " 'know',\n",
       " 'have',\n",
       " 'say',\n",
       " 'be',\n",
       " 'know',\n",
       " 'allow',\n",
       " 'know',\n",
       " 'be',\n",
       " 'refuse',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'be',\n",
       " 'say',\n",
       " 'keep',\n",
       " 'fix',\n",
       " 'felt',\n",
       " 'catch',\n",
       " 'do',\n",
       " 'get',\n",
       " 'continue',\n",
       " 'turn',\n",
       " 'speak',\n",
       " 'say',\n",
       " 'doesn',\n",
       " 'seem',\n",
       " 'dry',\n",
       " 'say',\n",
       " 'want',\n",
       " 'know',\n",
       " 'have',\n",
       " 'pause',\n",
       " 'think',\n",
       " 'speak',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'say',\n",
       " 'point',\n",
       " 'crowd',\n",
       " 'call',\n",
       " 'have',\n",
       " 'do',\n",
       " 'put',\n",
       " 'pull',\n",
       " 'have',\n",
       " 'get',\n",
       " 'hand',\n",
       " 'go',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'think',\n",
       " 'look',\n",
       " 'do',\n",
       " 'dare',\n",
       " 'think',\n",
       " 'say',\n",
       " 'bow',\n",
       " 'take',\n",
       " 'look',\n",
       " 'promise',\n",
       " 'tell',\n",
       " 'know',\n",
       " 'say',\n",
       " 'be',\n",
       " 'add',\n",
       " 'afraid',\n",
       " 'be',\n",
       " 'offend',\n",
       " 'say',\n",
       " 'turn',\n",
       " '_is_',\n",
       " 'say',\n",
       " 'look',\n",
       " 'do',\n",
       " 'call',\n",
       " 'say',\n",
       " 'beg',\n",
       " 'say',\n",
       " 'have',\n",
       " 'get',\n",
       " 'think',\n",
       " 'say',\n",
       " 'make',\n",
       " 'look',\n",
       " 'plead',\n",
       " 'call',\n",
       " 'join',\n",
       " 'please',\n",
       " 'do',\n",
       " 'say',\n",
       " 'address',\n",
       " 'reply',\n",
       " 'be',\n",
       " 'talk',\n",
       " 'move',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'cry',\n",
       " 'felt',\n",
       " 'guess',\n",
       " 'be',\n",
       " 'look',\n",
       " 'begin',\n",
       " 'hunt',\n",
       " 'be',\n",
       " 'be',\n",
       " 'seeneverything',\n",
       " 'seem',\n",
       " 'have',\n",
       " 'change',\n",
       " 'have',\n",
       " 'vanish',\n",
       " 'go',\n",
       " 'hunt',\n",
       " 'call',\n",
       " 'do',\n",
       " 'be',\n",
       " 'frighten',\n",
       " 'run',\n",
       " 'point',\n",
       " 'try',\n",
       " 'explain',\n",
       " 'have',\n",
       " 'make',\n",
       " 'queer',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'be',\n",
       " 'go',\n",
       " 'begin',\n",
       " 'fancy',\n",
       " 'happen',\n",
       " 'don',\n",
       " 'go',\n",
       " 'stop',\n",
       " 'begin',\n",
       " 'order',\n",
       " 'have',\n",
       " 'have',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'get',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'think',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'order',\n",
       " 'think',\n",
       " '_never_',\n",
       " 'get',\n",
       " 'be',\n",
       " 'know',\n",
       " 'be',\n",
       " 'come',\n",
       " 'look',\n",
       " 'tremble',\n",
       " 'shake',\n",
       " 'forget',\n",
       " 'be',\n",
       " 'have',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'come',\n",
       " 'try',\n",
       " 'open',\n",
       " 'open',\n",
       " 'be',\n",
       " 'press',\n",
       " 'prove',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'go',\n",
       " 'get',\n",
       " 'think',\n",
       " 'wait',\n",
       " 'fancy',\n",
       " 'hear',\n",
       " 'spread',\n",
       " 'make',\n",
       " 'be',\n",
       " 'hear',\n",
       " 'don',\n",
       " 'think',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'come',\n",
       " 'think',\n",
       " 'knowNo',\n",
       " 'fluster',\n",
       " 'tell',\n",
       " 'know',\n",
       " 'be',\n",
       " 'come',\n",
       " 'go',\n",
       " 'say',\n",
       " 'call',\n",
       " 'do',\n",
       " 'set',\n",
       " 'be',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'wonder',\n",
       " '_will_',\n",
       " 'do',\n",
       " 'begin',\n",
       " 'move',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'do',\n",
       " 'begin',\n",
       " 'have',\n",
       " 'doubt',\n",
       " 'come',\n",
       " 'rattle',\n",
       " 'hit',\n",
       " 'notice',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'lay',\n",
       " 'come',\n",
       " 'make',\n",
       " 'appear',\n",
       " 'run',\n",
       " 'find',\n",
       " 'get',\n",
       " 'do',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'wander',\n",
       " 'be',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'find',\n",
       " 'say',\n",
       " 'coax',\n",
       " 'try',\n",
       " 'whistle',\n",
       " 'be',\n",
       " 'frighten',\n",
       " 'be',\n",
       " 'be',\n",
       " 'eat',\n",
       " 'know',\n",
       " 'do',\n",
       " 'pick',\n",
       " 'hold',\n",
       " 'jump',\n",
       " 'rush',\n",
       " 'make',\n",
       " 'worry',\n",
       " 'dodge',\n",
       " 'keep',\n",
       " 'be',\n",
       " 'run',\n",
       " 'appear',\n",
       " 'make',\n",
       " 'tumble',\n",
       " 'get',\n",
       " 'think',\n",
       " 'be',\n",
       " 'have',\n",
       " 'expect',\n",
       " 'be',\n",
       " 'trample',\n",
       " 'run',\n",
       " 'begin',\n",
       " 'run',\n",
       " 'bark',\n",
       " 'sit',\n",
       " 'pant',\n",
       " 'hang',\n",
       " 'seem',\n",
       " 'Alice',\n",
       " 'make',\n",
       " 'set',\n",
       " 'run',\n",
       " 'be',\n",
       " 'till',\n",
       " 'sound',\n",
       " 'say',\n",
       " 'lean',\n",
       " 'rest',\n",
       " 'fan',\n",
       " 'have',\n",
       " 'like',\n",
       " 'teach',\n",
       " 'trick',\n",
       " 'd',\n",
       " 'be',\n",
       " 'do',\n",
       " 'look',\n",
       " 'do',\n",
       " 'see',\n",
       " 'look',\n",
       " 'eat',\n",
       " 'drink',\n",
       " 'look',\n",
       " 'take',\n",
       " 'address',\n",
       " 'reply',\n",
       " 'know',\n",
       " 'know',\n",
       " '_was_',\n",
       " 'get',\n",
       " 'think',\n",
       " 'have',\n",
       " 'be',\n",
       " 'change',\n",
       " 'say',\n",
       " 'see',\n",
       " 'put',\n",
       " 'reply',\n",
       " 'begin',\n",
       " 'be',\n",
       " 'be',\n",
       " 'haven',\n",
       " 'find',\n",
       " 'say',\n",
       " 'have',\n",
       " 'turn',\n",
       " 'knowand',\n",
       " 'think',\n",
       " 'win',\n",
       " 'be',\n",
       " 'say',\n",
       " 'know',\n",
       " 'be',\n",
       " 'feel',\n",
       " '_me_',\n",
       " 'felt',\n",
       " 'make',\n",
       " 'draw',\n",
       " 'say',\n",
       " 'think',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'be',\n",
       " 'think',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'sound',\n",
       " 'turn',\n",
       " 'come',\n",
       " 'say',\n",
       " 'swallow',\n",
       " 'think',\n",
       " 'have',\n",
       " 'do',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'say',\n",
       " 'remember',\n",
       " 'usedand',\n",
       " 'don',\n",
       " 'keep',\n",
       " 'reply',\n",
       " 'fold',\n",
       " 'begin',\n",
       " 'be',\n",
       " 'say',\n",
       " 'have',\n",
       " 'become',\n",
       " 'stand',\n",
       " 'Do',\n",
       " 'think',\n",
       " 'be',\n",
       " '_quite_',\n",
       " 'say',\n",
       " 'have',\n",
       " 'get',\n",
       " 'alter',\n",
       " 'particular',\n",
       " 'reply',\n",
       " 'change',\n",
       " 'know',\n",
       " 'say',\n",
       " 'have',\n",
       " 'be',\n",
       " 'contradict',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'lose',\n",
       " 'like',\n",
       " 'be',\n",
       " 'wouldn',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'plead',\n",
       " 'wait',\n",
       " 'choose',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'remain',\n",
       " 'look',\n",
       " 'try',\n",
       " 'make',\n",
       " 'be',\n",
       " 'be',\n",
       " 'round',\n",
       " 'find',\n",
       " 'say',\n",
       " 'change',\n",
       " 'find',\n",
       " 'be',\n",
       " 'be',\n",
       " 'find',\n",
       " 'see',\n",
       " 'look',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'rise',\n",
       " 'lay',\n",
       " 'say',\n",
       " 'say',\n",
       " 'haven',\n",
       " 're',\n",
       " 'talk',\n",
       " 'say',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'say',\n",
       " 'have',\n",
       " 'finish',\n",
       " 'be',\n",
       " 'annoy',\n",
       " 'say',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'see',\n",
       " 'say',\n",
       " 'm',\n",
       " 'say',\n",
       " 'remember',\n",
       " 'have',\n",
       " 'go',\n",
       " '_have_',\n",
       " 'taste',\n",
       " 'say',\n",
       " 'be',\n",
       " 'do',\n",
       " 'know',\n",
       " 'be',\n",
       " 'be',\n",
       " 'give',\n",
       " 'add',\n",
       " 'look',\n",
       " 'know',\n",
       " 'do',\n",
       " 'matter',\n",
       " 're',\n",
       " 'matter',\n",
       " '_me_',\n",
       " 'say',\n",
       " 'look',\n",
       " 'happen',\n",
       " 'be',\n",
       " 'shouldn',\n",
       " 'want',\n",
       " 'don',\n",
       " 'raw',\n",
       " 'crouch',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'entangle',\n",
       " 'have',\n",
       " 'stop',\n",
       " 'untwist',\n",
       " 'live',\n",
       " 'think',\n",
       " 'll',\n",
       " 'do',\n",
       " 'come',\n",
       " '_this_',\n",
       " 'frighten',\n",
       " 'be',\n",
       " 'open',\n",
       " 'notice',\n",
       " 'have',\n",
       " 'powder',\n",
       " 'curl',\n",
       " 'laugh',\n",
       " 'have',\n",
       " 'run',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'go',\n",
       " 'be',\n",
       " 'sit',\n",
       " 'star',\n",
       " 'go',\n",
       " 'knock',\n",
       " 'say',\n",
       " 'be',\n",
       " 'get',\n",
       " 'be',\n",
       " 'look',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'ask',\n",
       " 'be',\n",
       " 'do',\n",
       " 'be',\n",
       " 'tell',\n",
       " 'say',\n",
       " 'talk',\n",
       " 'say',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'sneeze',\n",
       " 'tell',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'say',\n",
       " 'jump',\n",
       " 'saw',\n",
       " 'be',\n",
       " 'address',\n",
       " 'take',\n",
       " 'go',\n",
       " 'didn',\n",
       " 'know',\n",
       " 'grin',\n",
       " 'didn',\n",
       " 'know',\n",
       " '_could_',\n",
       " 'don',\n",
       " 'do',\n",
       " 'say',\n",
       " 'feel',\n",
       " 'please',\n",
       " 'have',\n",
       " 'get',\n",
       " 'do',\n",
       " 'think',\n",
       " 'be',\n",
       " 'introduce',\n",
       " 'cry',\n",
       " 'jump',\n",
       " '_not_',\n",
       " 'be',\n",
       " 'say',\n",
       " 'felt',\n",
       " 'get',\n",
       " 'show',\n",
       " 'glance',\n",
       " 'see',\n",
       " 'mean',\n",
       " 'take',\n",
       " 'be',\n",
       " 'stir',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'go',\n",
       " '_think_',\n",
       " 'be',\n",
       " 'twelve',\n",
       " 'sing',\n",
       " 'keep',\n",
       " 'toss',\n",
       " 'howl',\n",
       " 'hear',\n",
       " 'speak',\n",
       " 'beat',\n",
       " 'sneeze',\n",
       " 'thoroughly',\n",
       " 'enjoy',\n",
       " 'please',\n",
       " 'say',\n",
       " 'fling',\n",
       " 'speak',\n",
       " 'catch',\n",
       " 'be',\n",
       " 'hold',\n",
       " 'don',\n",
       " 'take',\n",
       " 'kill',\n",
       " 't',\n",
       " 'be',\n",
       " 'murder',\n",
       " 'leave',\n",
       " 'say',\n",
       " 'express',\n",
       " 'grunt',\n",
       " 'look',\n",
       " 'see',\n",
       " 'be',\n",
       " 'be',\n",
       " 'have',\n",
       " 'be',\n",
       " 'get',\n",
       " 'do',\n",
       " 're',\n",
       " 'go',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'have',\n",
       " 'do',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'be',\n",
       " 'do',\n",
       " 'get',\n",
       " 'grin',\n",
       " 'saw',\n",
       " 'please',\n",
       " 'go',\n",
       " 'don',\n",
       " 'say',\n",
       " 'so',\n",
       " 'get',\n",
       " 'add',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'deny',\n",
       " 'try',\n",
       " 'don',\n",
       " 'want',\n",
       " 'go',\n",
       " 'remark',\n",
       " 'say',\n",
       " 'think',\n",
       " 'prove',\n",
       " 'go',\n",
       " 'do',\n",
       " 'know',\n",
       " 're',\n",
       " 'suppose',\n",
       " 'say',\n",
       " 'purr',\n",
       " 'growl',\n",
       " 'say',\n",
       " 'like',\n",
       " 'say',\n",
       " 'haven',\n",
       " 'be',\n",
       " 'invite',\n",
       " 'be',\n",
       " 'surprise',\n",
       " 'be',\n",
       " 'get',\n",
       " 'use',\n",
       " 'queer',\n",
       " 'happen',\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = []\n",
    "for word_arr in verbdict:\n",
    "    for word in word_arr:\n",
    "        word = re.sub(r\"[^\\w\\s]\", \"\", word, re.UNICODE)\n",
    "        if word and word != \"s\":\n",
    "            verbs.append(word)\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 268),\n",
       " ('say', 198),\n",
       " ('have', 103),\n",
       " ('think', 74),\n",
       " ('go', 65),\n",
       " ('do', 48),\n",
       " ('get', 46),\n",
       " ('look', 44),\n",
       " ('begin', 32),\n",
       " ('know', 31)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actions she most often did\n",
    "fdist = FreqDist(nltk.Text(verbs))\n",
    "fdist.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1991ae5a1b0be0d62723ce64b41daebbd092507e11a2eda874c09458c9f791ee"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
